# NLPcontest

> some note used in the NLPcontest

link of contest :[**å…¨çƒäººå·¥æ™ºèƒ½æŠ€æœ¯åˆ›æ–°å¤§èµ›**çƒ­èº«èµ›äºŒ: ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹æ³›åŒ–èƒ½åŠ›æŒ‘æˆ˜èµ›](https://tianchi.aliyun.com/competition/entrance/531865/introduction)

---

## å­¦ä¹ èµ„æ–™

* [è¸©å‘è®°å½•â€”â€”è®°ä¸€æ¬¡è®­ç»ƒæäº¤baselineå…¨è¿‡ç¨‹](https://blog.csdn.net/weixin_40807714/article/details/113856151)
* [æ¯”èµ›å…¨æµç¨‹ä½“éªŒ--datawhale](https://github.com/finlay-liu/tianchi-multi-task-nlp/blob/main/README.md#%E6%AF%94%E8%B5%9B%E5%85%A8%E6%B5%81%E7%A8%8B%E4%BD%93%E9%AA%8C)

## ç–‘é—®è®°å½•

---

ä»€ä¹ˆæ˜¯PyTorch?:bç«™ï¼Œå®˜ç½‘ï¼Ÿ





## å…¨æµç¨‹ä½“éªŒ

---

### ä¸‹è½½Bertå…¨æƒé‡

* [**Hugging Face**](https://huggingface.co/)å¦‚ä½•clone ä»“åº“ï¼špowershell åˆ°è¾¾æ–‡ä»¶å¤¹è¾“å…¥`git clone https://huggingface.co/bert-base-chinese`

  >How to use from the [ğŸ¤—/transformers](https://github.com/huggingface/transformers) library:

  

  ```
  from transformers import AutoTokenizer, AutoModelForMaskedLM
  
  tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
  
  model = AutoModelForMaskedLM.from_pretrained("bert-base-chinese")
  ```

  ### Or just clone the model repo

  ```
  git lfs install
  git clone https://huggingface.co/bert-base-chinese
  
  # if you want to clone without large files â€“ just their pointers
  # prepend your git clone with the following env var:
  
  GIT_LFS_SKIP_SMUDGE=1
  ```
  * é‡åˆ°çš„é—®é¢˜1ï¼šwarning: Clone succeeded, but checkout failed.
    * å¤„ç†ï¼šæ²¡æœ‰å‘ç°æœ‰ä»€ä¹ˆå½±å“ï¼Œæ–‡ä»¶å·²ç»ä¸‹è½½æˆåŠŸäº†
  * é—®é¢˜2ï¼šä¸¤ä¸ªå¤§æ–‡ä»¶æ²¡æœ‰clone ä¸‹æ¥ï¼Œæ‰‹åŠ¨ä¸‹è½½ï¼Œä½¿ç”¨xdownä¸‹è½½å¾ˆå¿«
    * pytorch_model.bin
    * tf_model.h5

* config.json vocab.txt pytorch_model.binï¼ŒæŠŠè¿™ä¸‰ä¸ªæ–‡ä»¶æ”¾è¿›tianchi-multi-task-nlp/bert_pretrain_modelæ–‡ä»¶å¤¹ä¸‹ã€‚

### æ•°æ®é›†ä¸‹è½½

ä½¿ç”¨powershell å¿«é€Ÿåˆ›å»ºæ–‡ä»¶å¤¹

## 3åˆ†å¼€è®­ç»ƒé›†å’ŒéªŒè¯é›†

* ç›´æ¥forker [æ¯”èµ›å…¨æµç¨‹ä½“éªŒ--datawhale ](https://github.com/finlay-liu/tianchi-multi-task-nlp/blob/main/README.md#%E6%AF%94%E8%B5%9B%E5%85%A8%E6%B5%81%E7%A8%8B%E4%BD%93%E9%AA%8C)

* ç„¶åclone åˆ°æœ¬åœ°

  > å¦‚ä½•å–æ¶ˆæœ¬åœ°ä»“åº“ï¼Œåˆ é™¤.git æ–‡ä»¶å¤¹ã€‚
  >
  > å¦‚ä½•åœ¨æ–‡ä»¶ç®¡ç†å™¨ä¸­æŸ¥çœ‹.git æ–‡ä»¶å¤¹ï¼Œæ–‡ä»¶èµ„æºç®¡ç†å™¨â€œæŸ¥çœ‹â€-ã€‹â€œéšè—çš„é¡¹ç›®â€

* æ–‡ä»¶ç§»åŠ¨

> note:ä¹‹å‰æ˜¯å…ˆcloneåˆ°æœ¬åœ°ï¼Œç„¶åæ–‡ä»¶éƒ½å·²ç»æ”¾è¿›æ–‡ä»¶å¤¹ã€‚ä½†æ˜¯åæ¥æƒ³åˆ°forkä¹‹åå¯ä»¥åŒæ­¥åˆ°è‡ªå·±çš„åŒæ­¥ï¼Œå¯ä»¥å­˜å‚¨ä¿®æ”¹ã€‚äºæ˜¯ï¼š1.åˆ é™¤äº†åŸæ¥çš„æ‰€æœ‰æ–‡ä»¶å¤¹2.é‡æ–°cloneã€‚ ä½†æ˜¯ï¼šåæ¥æƒ³åˆ°å¯ä»¥ä¸åˆ é™¤ï¼Œç›´æ¥forker ä¹‹åå†pushä¹Ÿå¯ä»¥

* è¿è¡Œ

```python
python ./generate_data.py
```

é—®é¢˜ï¼š

```
-------------------------------start-----------------------------------
Traceback (most recent call last):
  File "./generate_data.py", line 91, in <module>
    split_dataset(dev_data_cnt=3000)
  File "./generate_data.py", line 20, in split_dataset
    for line in f:
UnicodeDecodeError: 'gbk' codec can't decode byte 0xaf in position 6: illegal multibyte sequence
```

æ‰€æœ‰çš„with open éƒ½åŠ ä¸Š encoding='utf-8'

é—®é¢˜2ï¼šæœ¬åœ°æ²¡æœ‰GPUè¿è¡Œï¼Œå­¦ä¹ äº‘ç«¯çš„å¦‚ä½•ä½¿ç”¨ï¼Ÿ

* çœ‹ç›´æ’­è§†é¢‘

  > å¤©æ± ç«èµ›dockerå¿«é€Ÿå¤ç°ï¼š
  > å½•æ’­åœ°å€ï¼šhttps://www.bilibili.com/video/BV1jy4y1J7rp/
  > è§†é¢‘ææ–™é“¾æ¥ï¼šhttps://pan.baidu.com/s/1LWcYRa4cpz5AkDeTeebqvg 
  > æå–ç ï¼š1234 
  > ------------------------------
  > å¤©æ± nlpé¢„è®­ç»ƒæ³›åŒ–æŒ‘æˆ˜Baselineï¼š
  > å½•æ’­åœ°å€ï¼šhttps://www.bilibili.com/video/BV1Yz4y127FM/
  > è§†é¢‘ææ–™é“¾æ¥ï¼šé“¾æ¥ï¼šhttps://pan.baidu.com/s/13QPXvfryu6sXopJQ04gFnA 
  > æå–ç ï¼š1234 

* [å­¦ä¹ google çš„python æ€ä¹ˆç”¨](https://cloud.google.com/ai-platform/training/docs/using-gpus?hl=zh-cn)

## 4.æäº¤

æ—¥æœŸ:2021-02-23 10:37:16æ’å: æ— [æŸ¥çœ‹æ—¥å¿—](https://tianchi.aliyun.com/competition/examinelog/531865/723/443409/1286355?isTcc=true&scoreId=1577767)

score:0.6172

score_ocnli:0.7010

score_tnews:0.6867

score_ocemotion:0.4639