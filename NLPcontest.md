# NLPcontest

> some note used in the NLPcontest

link of contest :[**å…¨çƒäººå·¥æ™ºèƒ½æŠ€æœ¯åˆ›æ–°å¤§èµ›**çƒ­èº«èµ›äºŒ: ä¸­æ–‡é¢„è®­ç»ƒæ¨¡åž‹æ³›åŒ–èƒ½åŠ›æŒ‘æˆ˜èµ›](https://tianchi.aliyun.com/competition/entrance/531865/introduction)

---

## å­¦ä¹ èµ„æ–™

* [è¸©å‘è®°å½•â€”â€”è®°ä¸€æ¬¡è®­ç»ƒæäº¤baselineå…¨è¿‡ç¨‹](https://blog.csdn.net/weixin_40807714/article/details/113856151)
* [æ¯”èµ›å…¨æµç¨‹ä½“éªŒ--datawhale](https://github.com/finlay-liu/tianchi-multi-task-nlp/blob/main/README.md#%E6%AF%94%E8%B5%9B%E5%85%A8%E6%B5%81%E7%A8%8B%E4%BD%93%E9%AA%8C)

## ç–‘é—®è®°å½•

---

ä»€ä¹ˆæ˜¯PyTorch?:bç«™ï¼Œå®˜ç½‘ï¼Ÿ





## å…¨æµç¨‹ä½“éªŒ

---

### ä¸‹è½½Bertå…¨æƒé‡

* [**Hugging Face**](https://huggingface.co/)å¦‚ä½•clone ä»“åº“ï¼špowershell åˆ°è¾¾æ–‡ä»¶å¤¹è¾“å…¥`git clone https://huggingface.co/bert-base-chinese`

  >How to use from the [ðŸ¤—/transformers](https://github.com/huggingface/transformers) library:

  

  ```
  from transformers import AutoTokenizer, AutoModelForMaskedLM
  
  tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
  
  model = AutoModelForMaskedLM.from_pretrained("bert-base-chinese")
  ```

  ### Or just clone the model repo

  ```
  git lfs install
  git clone https://huggingface.co/bert-base-chinese
  
  # if you want to clone without large files â€“ just their pointers
  # prepend your git clone with the following env var:
  
  GIT_LFS_SKIP_SMUDGE=1
  ```
  * é‡åˆ°çš„é—®é¢˜1ï¼šwarning: Clone succeeded, but checkout failed.
    * å¤„ç†ï¼šæ²¡æœ‰å‘çŽ°æœ‰ä»€ä¹ˆå½±å“ï¼Œæ–‡ä»¶å·²ç»ä¸‹è½½æˆåŠŸäº†
  * é—®é¢˜2ï¼šä¸¤ä¸ªå¤§æ–‡ä»¶æ²¡æœ‰clone ä¸‹æ¥ï¼Œæ‰‹åŠ¨ä¸‹è½½ï¼Œä½¿ç”¨xdownä¸‹è½½å¾ˆå¿«
    * pytorch_model.bin
    * tf_model.h5

* config.json vocab.txt pytorch_model.binï¼ŒæŠŠè¿™ä¸‰ä¸ªæ–‡ä»¶æ”¾è¿›tianchi-multi-task-nlp/bert_pretrain_modelæ–‡ä»¶å¤¹ä¸‹ã€‚

### æ•°æ®é›†ä¸‹è½½

ä½¿ç”¨powershell å¿«é€Ÿåˆ›å»ºæ–‡ä»¶å¤¹

## 3åˆ†å¼€è®­ç»ƒé›†å’ŒéªŒè¯é›†



ï¼Ÿï¼Ÿï¼Ÿå¯¹å“¦ï¼Œè‡ªå·±è¿žNLPæ˜¯å•¥å¥½åƒéƒ½ä¸æ‡‚ã€‚ã€‚åŽ»æ‰¾æ‰¾æ•™ç¨‹ï¼Œç­‰ç­‰ï¼Œå› è¯¥å’Œä¹‹å‰çš„ç¥žç»ç½‘ç»œå’Œè§‰é”™æ ‘å·®ä¸å¤šï¼Œè°ƒç”¨ä¸€ä¸ªå‡½æ•°ï¼Ÿï¼Ÿå…ˆçœ‹ä¸€ä¸‹æ˜¯ä¸æ˜¯è¦ç”¨ç¬¬ä¸€ä¸ªä¸‹è½½çš„åŒ…ï¼Œçœ‹çœ‹æ€Žä¹ˆç”¨ï¼Œæ‰¾æ‰¾æ•™ç¨‹

> >1. åˆ†å¼€è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œé»˜è®¤éªŒè¯é›†æ˜¯å„3000æ¡æ•°æ®ï¼Œå‚æ•°å¯ä»¥è‡ªå·±ä¿®æ”¹ï¼š
> >
> >```
> >python ./generate_data.py
> >```
> >
> >1. è®­ç»ƒæ¨¡åž‹ï¼Œä¸€ä¸ªepochï¼š
> >
> >```
> >python ./train.py
> >```
> >
> >ä¼šä¿å­˜éªŒè¯é›†ä¸Šå¹³å‡f1åˆ†æ•°æœ€é«˜çš„æ¨¡åž‹åˆ° ./saved_best.pt
> >
> >1. ç”¨è®­ç»ƒå¥½çš„æ¨¡åž‹ ./saved_best.pt ç”Ÿæˆç»“æžœï¼š
> >
> >```
> >python ./inference.py
> >```
> >
> >1. æ‰“åŒ…é¢„æµ‹ç»“æžœã€‚
> >
> >```
> >zip -r ./result.zip ./*.json
> >```